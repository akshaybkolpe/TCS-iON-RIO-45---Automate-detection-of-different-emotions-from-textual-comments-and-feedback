{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sClUuUxRUUC1"
   },
   "source": [
    "**Step 1: Setting up the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Uwcbij3KQRE",
    "outputId": "3549492a-d987-481e-dce3-a4f4aa798d6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU1GMUesU4to"
   },
   "source": [
    "**Step 2: Choosing our Dataset**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiZNURGjQH0Y"
   },
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "RtCZpodELFH8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>ShansBee</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>nic0lepaula</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Ingenue_Em</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id  emotion       author  \\\n",
       "0  1956967666  sadness    wannamama   \n",
       "1  1956967696  sadness    coolfunky   \n",
       "2  1956968487  sadness     ShansBee   \n",
       "3  1956969035  sadness  nic0lepaula   \n",
       "4  1956969172  sadness   Ingenue_Em   \n",
       "\n",
       "                                             content  \n",
       "0  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "1                Funeral ceremony...gloomy friday...  \n",
       "2  I should be sleep, but im not! thinking about ...  \n",
       "3            @charviray Charlene my love. I miss you  \n",
       "4         @kelcouch I'm sorry  at least it's Friday?  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('text_emotion.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjh3n-d7QAu9"
   },
   "source": [
    " Dropping rows with other emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-h7CNFsKLPKi"
   },
   "outputs": [],
   "source": [
    "data = data.drop(data[data.emotion == 'anger'].index)\n",
    "data = data.drop(data[data.emotion == 'boredom'].index)\n",
    "data = data.drop(data[data.emotion == 'enthusiasm'].index)\n",
    "data = data.drop(data[data.emotion == 'empty'].index)\n",
    "data = data.drop(data[data.emotion == 'fun'].index)\n",
    "data = data.drop(data[data.emotion == 'relief'].index)\n",
    "data = data.drop(data[data.emotion == 'surprise'].index)\n",
    "data = data.drop(data[data.emotion == 'love'].index)\n",
    "data = data.drop(data[data.emotion == 'hate'].index)\n",
    "data = data.drop(data[data.emotion == 'neutral'].index)\n",
    "data = data.drop(data[data.emotion == 'worry'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEwQ4QJoVUOF"
   },
   "source": [
    "**Step 3: Preprocessing the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXQ4LXvpQVrd"
   },
   "source": [
    "Making all letters lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WO7uQBNULaSY"
   },
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBUyM_IqQZn5"
   },
   "source": [
    "Removing Punctuation, Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQwtF6UkLmZ4",
    "outputId": "606eed62-b7c2-4b42-c0d5-3c97566025ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_2552/2143674117.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['content'] = data['content'].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "data['content'] = data['content'].str.replace('[^\\w\\s]',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NadY5KiwQfE8"
   },
   "source": [
    " Removing Stop Words using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nemF1wgKLuSF"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtbuiCGkQmVH"
   },
   "source": [
    "Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lnuRtVZOMUhH"
   },
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zblS68EKQxYr"
   },
   "source": [
    "Correcting Letter Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mQVl6_GcMtLH"
   },
   "outputs": [],
   "source": [
    "def de_repeat(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wqmUpDt3MvwP"
   },
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(lambda x: \" \".join(de_repeat(x) for x in x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpzIBTYuQ_gR"
   },
   "source": [
    "Code to find the top 10,000 rarest words appearing in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "nUiRoz0_M1Yv"
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(data['content']).split()).value_counts()[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw3AsHP4RHZk"
   },
   "source": [
    "Removing all those rarely appearing words from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "UxO1QospM6CZ"
   },
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "data['content'] = data['content'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4AceRIlVopE"
   },
   "source": [
    "**Step 4: Feature Extraction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMFcDXCwRJWa"
   },
   "source": [
    "Encoding output labels 'sadness' as '1' & 'happiness' as '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ayIJzLmwND56"
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data.emotion.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDGvxkgDRRJL"
   },
   "source": [
    "Splitting into training and testing data in 90:10 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "KlWQ_j_SNJ9_"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data.content.values, y, stratify=y, random_state=42, test_size=0.1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJvjW5cVRWec"
   },
   "source": [
    " Extracting TF-IDF parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wBmIBH9kNQSI"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_Q6u0VRRa6v"
   },
   "source": [
    " Extracting Count Vectors Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "TtTmIggkNVc0"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data['content'])\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_val_count =  count_vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpF8t-cKVx7P"
   },
   "source": [
    "**Step 5: Training Our Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8PccXW6Rf71"
   },
   "source": [
    "Model 1: Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__FVLI58NdnS",
    "outputId": "1d55261f-edaf-465f-fe39-76db7f3fd889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.48265895953757226\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_val_tfidf)\n",
    "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUr0gJQGRoMe"
   },
   "source": [
    "Model 2: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSv8LutQNkH8",
    "outputId": "69e1bd4d-b99a-4031-98d8-560d97266ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm using tfidf accuracy 0.5096339113680154\n"
     ]
    }
   ],
   "source": [
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_tfidf, y_train)\n",
    "y_pred = lsvm.predict(X_val_tfidf)\n",
    "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xckPsxpIRuLu"
   },
   "source": [
    "Model 3: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwSRfN3JNrXt",
    "outputId": "3f3d7492-1cc4-4c68-9cdb-57edc1a3a33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg tfidf accuracy 0.4951830443159923\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_val_tfidf)\n",
    "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmZbP7JwR0yC"
   },
   "source": [
    "Model 4: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDvytbkiNy15",
    "outputId": "4739a5e7-bec7-446c-9201-3b20950a6223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest tfidf accuracy 0.4913294797687861\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_val_tfidf)\n",
    "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ScdR7aR496"
   },
   "source": [
    "**Building models using count vectors feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K--WDDDgSC9v"
   },
   "source": [
    "Model 1: Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eA8r37k9OBWg",
    "outputId": "23c70a99-d103-43c2-f5db-83dc90d2ebb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.7745664739884393\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred = nb.predict(X_val_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyFjPsnmSK9S"
   },
   "source": [
    "Model 2: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp2HBknjOLP-",
    "outputId": "ca20aefd-3c7a-4a9b-a592-adbff32a4d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvm using count vectors accuracy 0.7832369942196532\n"
     ]
    }
   ],
   "source": [
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlcSZGxuSQ_X"
   },
   "source": [
    "Model 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9UrR1QxOQvV",
    "outputId": "ad3c74f6-2d83-432a-ead1-e409709e2e95"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_count, y_train)\n",
    "y_pred = logreg.predict(X_val_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6D1zgU3SZ3G"
   },
   "source": [
    "Model 4: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXM1TKTGoiCs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOSF-Jq_OXi7",
    "outputId": "df04f54d-28ca-4107-b05f-5c775e0a0459"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_count, y_train)\n",
    "y_pred = rf.predict(X_val_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyiU3mgzSbft"
   },
   "source": [
    "Below are 8 random statements. The first 4 depict happiness. The last 4 depict sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r2KP4YjWOs7o"
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(['I am very happy today! The atmosphere looks cheerful',\n",
    "'Things are looking great. It was such a good day',\n",
    "'Success is right around the corner. Lets celebrate this victory',\n",
    "'Everything is more beautiful when you experience them with a smile!',\n",
    "'Now this is my worst, okay? But I am gonna get better.',\n",
    "'I am tired, boss. Tired of being on the road, lonely as a sparrow in the rain. I am tired of all the pain I feel',\n",
    "'This is quite depressing. I am filled with sorrow',\n",
    "'His death broke my heart. It was a sad day'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgD_T_FfSkam"
   },
   "source": [
    "Doing some preprocessing on these tweets as done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSBLgPlyO6my",
    "outputId": "200aaaf4-a5d5-430e-a7ca-3c5f1692fc1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_2552/2511845226.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "tweets[0] = tweets[0].str.replace('[^\\w\\s]',' ')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "from textblob import Word\n",
    "tweets[0] = tweets[0].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvq-XluYSuCP"
   },
   "source": [
    " Extracting Count Vectors feature from our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "46b0aplTPBWt"
   },
   "outputs": [],
   "source": [
    "tweet_count = count_vect.transform(tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlpLRnXCSwO5"
   },
   "source": [
    "Predicting the emotion of the tweet using our already trained linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_RmI6uBPGr8",
    "outputId": "941b3d65-179c-4f77-bb68-bd61ec55b1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "tweet_pred = lsvm.predict(tweet_count)\n",
    "print(tweet_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eIhejJhT1MJ"
   },
   "source": [
    "Note:Remember our encodings for the output. ‘0' is for happiness and ‘1’ is for sadness. Our model detected the emotion correctly for all the 8 sentences."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
